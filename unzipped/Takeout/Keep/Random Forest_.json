{"color":"DEFAULT","isTrashed":false,"isPinned":false,"isArchived":false,"textContent":"L'algorithme random Forrest va sélectionner au hasard plusieurs arbre de décisions.\n\ntrois hyper-paramètres principaux : \n- la taille des arbres (le nombre de nœuds maximal), \n- nombre d\u2019arbres à utiliser \n- le nombre de caractéristiques échantillonnées (nombre de variables aléatoires choisies à chaque mélange depuis les variables explicatives) \n\n La première étape consiste à appliquer le principe du bagging, c'est-à-dire créer de nombreux sous-échantillons aléatoires de notre ensemble de données avec possibilité de sélectionner la même valeur plusieurs fois.\n\nDes arbres de décision individuels sont ensuite construits pour chaque échantillon. Chaque arbre est entraîné sur une portion aléatoire afin de recréer une prédiction. Notons bien que ces modèles-là sont très peu corrélés, et chaque arbre de décision fonctionne individuellement et indépendamment des autres.\n\nPourquoi ceci est important ? C\u2019est parce que c'est la combinaison de tous ces modèles indépendants qui permettent de réduire la variance du modèle d'ensemble (plus stable, moins chaotique). En d\u2019autres termes, de corriger l\u2019instabilité des arbres de décision ( le fait que des modifications dans l'ensemble d'apprentissage peuvent engendrer des arbres très différents).\n\n3 - Enfin, chaque arbre va prédire un résultat (target). Le résultat avec le plus de votes ( le plus fréquent) devient le résultat final de notre modèle. Dans le cas de régression, on prendra la moyenne des votes de tous les arbres.","title":"Random Forest ","userEditedTimestampUsec":1698254277686000,"createdTimestampUsec":1698250342965000,"textContentHtml":"<p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">L'algorithme random Forrest va sélectionner au hasard plusieurs arbre de décisions.<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">trois hyper-paramètres principaux :&nbsp;<\/span><\/p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">- la taille des arbres (le nombre de nœuds maximal),&nbsp;<\/span><\/p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">- nombre d&rsquo;arbres à utiliser&nbsp;<\/span><\/p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">- le nombre de caractéristiques échantillonnées (nombre de variables aléatoires choisies à chaque mélange depuis les variables explicatives)&nbsp;<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;La première étape consiste à appliquer le principe du bagging, c'est-à-dire créer de nombreux sous-échantillons aléatoires de notre ensemble de données avec possibilité de sélectionner la même valeur plusieurs fois.<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Des arbres de décision individuels sont ensuite construits pour chaque échantillon. Chaque arbre est entraîné sur une portion aléatoire afin de recréer une prédiction. Notons bien que ces modèles-là sont très peu corrélés, et chaque arbre de décision fonctionne individuellement et indépendamment des autres.<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Pourquoi ceci est important ? C&rsquo;est parce que c'est la combinaison de tous ces modèles indépendants qui permettent de réduire la variance du modèle d'ensemble (plus stable, moins chaotique). En d&rsquo;autres termes, de corriger l&rsquo;instabilité des arbres de décision ( le fait que des modifications dans l'ensemble d'apprentissage peuvent engendrer des arbres très différents).<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">3 - Enfin, chaque arbre va prédire un résultat (target). Le résultat avec le plus de votes ( le plus fréquent) devient le résultat final de notre modèle. Dans le cas de régression, on prendra la moyenne des votes de tous les arbres.<\/span><\/p>","labels":[{"name":"Machine Learning"}]}