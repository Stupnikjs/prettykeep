{"color":"DEFAULT","isTrashed":false,"isPinned":false,"isArchived":false,"textContent":"One of the most popular dimension reduction techniques is PCA. Known as general factor analysis, PCA is useful for dramatically reducing data complexity and visualizing data in fewer dimensions.\n\n\nRather than removing individual features from the dataset, PCA recreates dimensions as a linear combination of features called components\n\nPCA can only be applied to continuous variables\n\n\nfrom ChatGPT \n\nEtape de PCA :\n\nNormalisation des moyennes : La première étape consiste à normaliser les données de manière à ce que chaque variable ait une moyenne de zéro. Cela est fait pour garantir que les variables sont à la même échelle et pour éviter qu'une variable n'ait une influence indue sur les résultats.\n\nCalcul de la matrice de covariance : La prochaine étape consiste à calculer la matrice de covariance des données normalisées. La matrice de covariance est une matrice carrée qui mesure la relation linéaire entre chaque paire de variables.\n\nCalcul des valeurs propres et des vecteurs propres : Les valeurs propres et les vecteurs propres de la matrice de covariance sont ensuite calculés. Les vecteurs propres sont les directions le long desquelles les données varient le plus, et les valeurs propres représentent l'amplitude de cette variation.\n\nSélection des composantes principales","title":"PCA","userEditedTimestampUsec":1675940527586000,"createdTimestampUsec":1675934860490000,"textContentHtml":"<p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">One of the most popular dimension reduction techniques is PCA. Known as general factor analysis, PCA is useful for dramatically reducing data complexity and visualizing data in fewer dimensions.<\/span><\/p><br /><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Rather than removing individual features from the dataset, PCA recreates dimensions as a linear combination of features called components<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">PCA can only be applied to continuous variables<\/span><\/p><br /><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">from ChatGPT&nbsp;<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Etape de PCA :<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Normalisation des moyennes : La première étape consiste à normaliser les données de manière à ce que chaque variable ait une moyenne de zéro. Cela est fait pour garantir que les variables sont à la même échelle et pour éviter qu'une variable n'ait une influence indue sur les résultats.<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Calcul de la matrice de covariance : La prochaine étape consiste à calculer la matrice de covariance des données normalisées. La matrice de covariance est une matrice carrée qui mesure la relation linéaire entre chaque paire de variables.<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Calcul des valeurs propres et des vecteurs propres : Les valeurs propres et les vecteurs propres de la matrice de covariance sont ensuite calculés. Les vecteurs propres sont les directions le long desquelles les données varient le plus, et les valeurs propres représentent l'amplitude de cette variation.<\/span><\/p><br /><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0.0pt;margin-bottom:0.0pt;\"><span style=\"font-size:16.0pt;font-family:'Google Sans';color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Sélection des composantes principales<\/span><\/p>","labels":[{"name":"Machine Learning"}]}